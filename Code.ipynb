{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1anlhdG-oWZP"
   },
   "source": [
    "#**Progetto DSIM**\n",
    "1. **Object Detection and Classification with** [YOLO3v](https://towardsdatascience.com/object-detection-using-yolov3-using-keras-80bf35e61ce1 )\n",
    "2. **Extracting crops** \n",
    "3. **Describing every crop** with:\n",
    "  - Identifier of original image\n",
    "  - Label of the crop\n",
    "  - Feature extracted on the crop\n",
    "    - SIFT\n",
    "    - VGG16 truncated\n",
    "    - YOLO model truncated\n",
    "  - Importance: $\\frac{A_{crop}}{A_{image}}$ \n",
    "4. **Generating vectors** for each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "seK8_65Igibn"
   },
   "source": [
    "**NB**\n",
    ">Per usare il metodo SIFT, prima di iniziare mandare i due codici citati nella sotto-sezione \"SIFT\" e reinizializzare il runtime.\n",
    "<br>\n",
    "\n",
    "> Caricare il modello coi pesi, eseguire il codice alla cella \"Define hyperparameters and target lables\" e partire direttamente dalla sezione 1.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQbKme0MnFhQ"
   },
   "outputs": [],
   "source": [
    "#Set tensorflow version\n",
    "\n",
    "# %tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEHqehFXobA_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "\n",
    "import os\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import struct\n",
    "import cv2\n",
    "from numpy import expand_dims\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Lambda, Conv2D, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, AveragePooling2D\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rfymsftnog5T"
   },
   "source": [
    "###**1. Object Detection and classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7fb2esBo1yC"
   },
   "source": [
    "####**1.1 Re-create Yolo3v Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIEx6qcZo7fd"
   },
   "source": [
    "- Define class for reading the pre-trained weights of YOLO3v model obtained on \"Coco\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtuwJuryodRC"
   },
   "outputs": [],
   "source": [
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major,    = struct.unpack('i', w_f.read(4))\n",
    "            minor,    = struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "\n",
    "            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                w_f.read(4)\n",
    "\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            \n",
    "            binary = w_f.read()\n",
    "\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "\n",
    "    def load_weights(self, model):\n",
    "        for i in range(106):\n",
    "            try:\n",
    "                conv_layer = model.get_layer('conv_' + str(i))\n",
    "                print(\"loading weights of convolution #\" + str(i))\n",
    "\n",
    "                if i not in [81, 93, 105]:\n",
    "                    norm_layer = model.get_layer('bnorm_' + str(i))\n",
    "\n",
    "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "\n",
    "                    beta  = self.read_bytes(size) # bias\n",
    "                    gamma = self.read_bytes(size) # scale\n",
    "                    mean  = self.read_bytes(size) # mean\n",
    "                    var   = self.read_bytes(size) # variance            \n",
    "\n",
    "                    weights = norm_layer.set_weights([gamma, beta, mean, var])  \n",
    "\n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    \n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel, bias])\n",
    "                else:\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel])\n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i))     \n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUbUVZAipS9t"
   },
   "source": [
    "- Create a function building the convolutional blocks, with or without skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38gqKuW1pBVa"
   },
   "outputs": [],
   "source": [
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    \n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        \n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'], \n",
    "                   conv['kernel'], \n",
    "                   strides=conv['stride'], \n",
    "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                   name='conv_' + str(conv['layer_idx']), \n",
    "                   use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "\n",
    "    return add([skip_connection, x]) if skip else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uekdrPgrpfuD"
   },
   "source": [
    "- Recreate the YOLO3v model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKQ5FrCUpfLI"
   },
   "outputs": [],
   "source": [
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(None, None, 3))\n",
    "\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "        \n",
    "    skip_36 = x\n",
    "        \n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "        \n",
    "    skip_61 = x\n",
    "        \n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "\n",
    "    # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "        \n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "\n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
    "                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "\n",
    "    # Layer 83 => 86\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_61])\n",
    "\n",
    "    # Layer 87 => 91\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "\n",
    "    # Layer 92 => 94\n",
    "    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
    "                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "\n",
    "    # Layer 95 => 98\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_36])\n",
    "\n",
    "    # Layer 99 => 106\n",
    "    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
    "                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "\n",
    "    model = Model(input_image, [yolo_82, yolo_94, yolo_106])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5eYZAAVprXR"
   },
   "source": [
    "- Define hyperparameters and target lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSDVjd1Kprnh"
   },
   "outputs": [],
   "source": [
    "net_h, net_w = 416, 416\n",
    "obj_thresh, nms_thresh = 0.5, 0.45\n",
    "anchors = [[116,90,  156,198,  373,326],  [30,61, 62,45,  59,119], [10,13,  16,30,  33,23]]\n",
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n",
    "              \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n",
    "              \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n",
    "              \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n",
    "              \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n",
    "              \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n",
    "              \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n",
    "              \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n",
    "              \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n",
    "              \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bulXmB1p221"
   },
   "source": [
    "- Call the empy architecture able to predict 80 classes on COCO and load the pre-trained weights into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aoa2zXhkp3GV",
    "outputId": "7f9f6a64-77ac-41ca-d67c-c3a351e1d4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fede/.anaconda/envs/dsim/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "loading weights of convolution #0\n",
      "loading weights of convolution #1\n",
      "loading weights of convolution #2\n",
      "loading weights of convolution #3\n",
      "no convolution #4\n",
      "loading weights of convolution #5\n",
      "loading weights of convolution #6\n",
      "loading weights of convolution #7\n",
      "no convolution #8\n",
      "loading weights of convolution #9\n",
      "loading weights of convolution #10\n",
      "no convolution #11\n",
      "loading weights of convolution #12\n",
      "loading weights of convolution #13\n",
      "loading weights of convolution #14\n",
      "no convolution #15\n",
      "loading weights of convolution #16\n",
      "loading weights of convolution #17\n",
      "no convolution #18\n",
      "loading weights of convolution #19\n",
      "loading weights of convolution #20\n",
      "no convolution #21\n",
      "loading weights of convolution #22\n",
      "loading weights of convolution #23\n",
      "no convolution #24\n",
      "loading weights of convolution #25\n",
      "loading weights of convolution #26\n",
      "no convolution #27\n",
      "loading weights of convolution #28\n",
      "loading weights of convolution #29\n",
      "no convolution #30\n",
      "loading weights of convolution #31\n",
      "loading weights of convolution #32\n",
      "no convolution #33\n",
      "loading weights of convolution #34\n",
      "loading weights of convolution #35\n",
      "no convolution #36\n",
      "loading weights of convolution #37\n",
      "loading weights of convolution #38\n",
      "loading weights of convolution #39\n",
      "no convolution #40\n",
      "loading weights of convolution #41\n",
      "loading weights of convolution #42\n",
      "no convolution #43\n",
      "loading weights of convolution #44\n",
      "loading weights of convolution #45\n",
      "no convolution #46\n",
      "loading weights of convolution #47\n",
      "loading weights of convolution #48\n",
      "no convolution #49\n",
      "loading weights of convolution #50\n",
      "loading weights of convolution #51\n",
      "no convolution #52\n",
      "loading weights of convolution #53\n",
      "loading weights of convolution #54\n",
      "no convolution #55\n",
      "loading weights of convolution #56\n",
      "loading weights of convolution #57\n",
      "no convolution #58\n",
      "loading weights of convolution #59\n",
      "loading weights of convolution #60\n",
      "no convolution #61\n",
      "loading weights of convolution #62\n",
      "loading weights of convolution #63\n",
      "loading weights of convolution #64\n",
      "no convolution #65\n",
      "loading weights of convolution #66\n",
      "loading weights of convolution #67\n",
      "no convolution #68\n",
      "loading weights of convolution #69\n",
      "loading weights of convolution #70\n",
      "no convolution #71\n",
      "loading weights of convolution #72\n",
      "loading weights of convolution #73\n",
      "no convolution #74\n",
      "loading weights of convolution #75\n",
      "loading weights of convolution #76\n",
      "loading weights of convolution #77\n",
      "loading weights of convolution #78\n",
      "loading weights of convolution #79\n",
      "loading weights of convolution #80\n",
      "loading weights of convolution #81\n",
      "no convolution #82\n",
      "no convolution #83\n",
      "loading weights of convolution #84\n",
      "no convolution #85\n",
      "no convolution #86\n",
      "loading weights of convolution #87\n",
      "loading weights of convolution #88\n",
      "loading weights of convolution #89\n",
      "loading weights of convolution #90\n",
      "loading weights of convolution #91\n",
      "loading weights of convolution #92\n",
      "loading weights of convolution #93\n",
      "no convolution #94\n",
      "no convolution #95\n",
      "loading weights of convolution #96\n",
      "no convolution #97\n",
      "no convolution #98\n",
      "loading weights of convolution #99\n",
      "loading weights of convolution #100\n",
      "loading weights of convolution #101\n",
      "loading weights of convolution #102\n",
      "loading weights of convolution #103\n",
      "loading weights of convolution #104\n",
      "loading weights of convolution #105\n"
     ]
    }
   ],
   "source": [
    "yolov3 = make_yolov3_model()\n",
    "\n",
    "weight_reader = WeightReader('yolov3.weights')\n",
    "weight_reader.load_weights(yolov3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ra66MyK96oPj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/.anaconda/envs/dsim/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "yolov3 = load_model(\"yolov3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oGjp6TwqS9D"
   },
   "source": [
    "- Plot the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "0kKncuL5qTM0",
    "outputId": "b992e67f-4bc6-40ac-9795-a2c92f48cbe8"
   },
   "outputs": [],
   "source": [
    "# plot_model(yolov3, to_file='model.png',rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRh-SrhYqdG6"
   },
   "source": [
    "####**1.2 Image Preprocessing**\n",
    "\n",
    "We need a function loading and preparing images to be elaborated by the R-CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0cXyDeNq6eu"
   },
   "outputs": [],
   "source": [
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    return image, width, height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZmzEEQ2rLtS"
   },
   "source": [
    "####**1.3 Bounding boxes extraction and management**\n",
    "\n",
    "In this section a class of functions is built for detecting target objects, identifying their location in the image through bounding boxes and computing their probability of actually belonging to the selected label - with a sigmoid function. The management of these boxes takes into account intersection over union (IoU) and the overlappings.\n",
    "<br>\n",
    "This class also consider a threshold of probability, beneath whom the bounding box is considered non maximal and thus suppressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EN_NTbgmrfME"
   },
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        \n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "            \n",
    "        return self.score\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3 \n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union\n",
    "\n",
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cU-pFD1ujBN"
   },
   "source": [
    "Then each NumPy array, one at a time, will be taken and the candidate bounding boxes and class predictions from the Yolov3 model will be decoded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsqYRFRAujTU"
   },
   "outputs": [],
   "source": [
    "def decode_netout(netout, anchors, obj_thresh,  net_h, net_w):\n",
    "    grid_h, grid_w = netout.shape[:2]\n",
    "    nb_box = 3\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "\n",
    "    boxes = []\n",
    "\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    "\n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i / grid_w\n",
    "        col = i % grid_w\n",
    "        \n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[int(row)][int(col)][b][4]\n",
    "            #objectness = netout[..., :4]\n",
    "            \n",
    "            if(objectness.all() <= obj_thresh): continue\n",
    "            \n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height  \n",
    "            \n",
    "            # last elements are class probabilities\n",
    "            classes = netout[int(row)][col][b][5:]\n",
    "            \n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "            #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)\n",
    "\n",
    "            boxes.append(box)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zu_x7G4vYzP"
   },
   "source": [
    "Bounding boxes will be stretched back into the shape of the original image, allowing the system to plot the original image and draw them above in order to detect real objects. Thus, `correct_yolo_boxes` corrects the sizes of the bounding boxes for the shape of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wO-yu85WvZBI"
   },
   "outputs": [],
   "source": [
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    if (float(net_w)/image_w) < (float(net_h)/image_h):\n",
    "        new_w = net_w\n",
    "        new_h = (image_h*net_w)/image_w\n",
    "    else:\n",
    "        new_h = net_w\n",
    "        new_w = (image_w*net_h)/image_h\n",
    "        \n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        \n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9Me6fExwMwd"
   },
   "source": [
    "Lastly, getting all of the results above a threshold,\n",
    "the list of boxes, known labels, and the classification threshold as arguments, `get_boxes` will return a parallel lists of boxes, labels, and scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1JBfQIUwNGO"
   },
   "outputs": [],
   "source": [
    "def get_boxes(boxes, labels, thresh):\n",
    "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
    "    # enumerate all boxes\n",
    "    for box in boxes:\n",
    "        # enumerate all possible labels\n",
    "        for i in range(len(labels)):\n",
    "            # check if the threshold for this label is high enough\n",
    "            if box.classes[i] > thresh:\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "                # don't break, many labels may trigger for one box\n",
    "    return v_boxes, v_labels, v_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJbSCZamxP_a"
   },
   "source": [
    "###**2. Cropping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "049uVonWxerC"
   },
   "source": [
    "- Extracting coordinates of every labelled object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yN1nxlxnxTMc"
   },
   "outputs": [],
   "source": [
    "def get_coord_boxes(data,v_boxes,v_labels):\n",
    "  l = []\n",
    "  lab = []\n",
    "  for i in range(len(v_boxes)):\n",
    "    box = v_boxes[i]\n",
    "    etichetta = v_labels[i]\n",
    "    y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "    lab.append(etichetta)\n",
    "    l.append([x1, y1, x2, y2])\n",
    "  df = pd.DataFrame(\n",
    "    {'Label': lab,\n",
    "     'Coordinates': l\n",
    "  })\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLMhT3iwyLs9"
   },
   "source": [
    "- Cropping interesting object at the coordinates indicated by the YOLO3v bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3E2jkmNyQmg"
   },
   "outputs": [],
   "source": [
    "def extract_coord(photo_filename,class_threshold = 0.6,obj_thresh = 0.5):\n",
    "  input_w, input_h = 416, 416\n",
    "  net_h, net_w = 416, 416\n",
    "  data = plt.imread(photo_filename)\n",
    "  image, image_w, image_h = load_image_pixels(photo_filename, (net_w, net_w))\n",
    "  anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
    "  # make prediction\n",
    "  yolos = yolov3.predict(image)\n",
    "  # define the anchors\n",
    "  boxes = list()\n",
    "  for i in range(len(yolos)):\n",
    "        # decode the output of the network\n",
    "        boxes += decode_netout(yolos[i][0], anchors[i], obj_thresh,  net_h, net_w)\n",
    "  \n",
    "  # correct the sizes of the bounding boxes\n",
    "  correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w)\n",
    "  \n",
    "  # suppress non-maximal boxes\n",
    "  do_nms(boxes, nms_thresh)\n",
    "  # get the details of the detected objects\n",
    "  v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "  dataframe = get_coord_boxes(data, v_boxes,v_labels)\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19SUByOUy-kO"
   },
   "source": [
    "###**3. Crops description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7RGHAeizZk2"
   },
   "source": [
    "####**3.1 Feature extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxhkGZsb0ID6"
   },
   "source": [
    "#####**3.1.1 SIFT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfK0eHaefvLr"
   },
   "source": [
    "**NB**: Per farlo funzione bisogna eseguire i seguenti codici e rinizializzare il runtime:\n",
    "<br>\n",
    "`!pip install opencv-python==3.4.2.16`\n",
    "<br>\n",
    " `!pip install opencv-contrib-python==3.4.2.16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U98g1IKgzlga"
   },
   "outputs": [],
   "source": [
    "def SIFT_extract(x,printed=False):\n",
    "  sift = cv2.xfeatures2d.SIFT_create()\n",
    "  (kps1, features1) = sift.detectAndCompute(x, None)\n",
    "  if printed:\n",
    "    _kps = cv2.drawKeypoints(x, kps1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(_kps); plt.show()\n",
    "  return features1 #set of descriptors of dimensione 128 for each kp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nShJabq1zai"
   },
   "source": [
    "**NB**: La similarità viene estratta applicando un [ratio test](https://python-forum.io/Thread-Score-of-similarity-using-SIFT-python) ai matches tra due immagini, ottenuti grazie alle features appena estratte: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfAts6Cog1nr"
   },
   "outputs": [],
   "source": [
    "# #Si crea il matcher\n",
    "# bf = cv2.BFMatcher(crossCheck=True)\n",
    "# #Si individuano i match\n",
    "# matches = bf.match(features1, features2)\n",
    "# #Si ordinano i matches\n",
    "# matches = sorted(matches, key=lambda x:x.distance)\n",
    "# \n",
    "# #Si applica il ratio test\n",
    "# good = []\n",
    "# for m,n in matches:\n",
    "#   if m.distance < 0.75*n.distance:\n",
    "#     good.append([m])\n",
    "#     a=len(good)\n",
    "#     percent=(a*100)/len(kp2)\n",
    "#     print(\"{} % similarity\".format(percent))\n",
    "#     if percent >= 75.00:\n",
    "#       print('Match Found')\n",
    "#     else:\n",
    "#       print('Match not Found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu-hta1y0R90"
   },
   "source": [
    "#####**3.1.2 YOLO3v**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "PR2DMf5fju3u",
    "outputId": "c9a77c25-c701-4e86-ff9f-4e8870397896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fede/.anaconda/envs/dsim/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction with pooling\n",
    "\n",
    "yolo_original = Model(inputs=yolov3.input, outputs=yolov3.get_layer(\"conv_103\").output)\n",
    "pool = AveragePooling2D(strides=(4, 4))(yolo_original.output)\n",
    "yolo_pooled = Model(inputs=yolov3.input, outputs=pool)\n",
    "\n",
    "#In case one does not to pool the features vectors, the model to be used is \"yolo_original\"\n",
    "#instead of \"yolo_pooled\"\n",
    "\n",
    "def feature_extract_YOLO(x, model = yolo_pooled):\n",
    "  x = Image.fromarray(x)\n",
    "  x = x.resize((224,224), Image.ANTIALIAS)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  f = model.predict(x) \n",
    "  return f.flatten() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fm4HviAy53MX"
   },
   "source": [
    "Using an Average Pooling method, the outputed vector will pass from a dimension of more than 1 mln to a dimension of 25088."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6myyUm8t0WJm"
   },
   "source": [
    "#####**3.1.1 VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtT3cMX50Ypm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fede/.anaconda/envs/dsim/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "\n",
    "def feature_extract_VGG16(x, model = VGG16(weights='imagenet', include_top=False, pooling='avg')):\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  #x = preprocess_input(x)\n",
    "  features = model.predict(x)\n",
    "  return features.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BTbioUVzh00"
   },
   "source": [
    "####**3.2 Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-F_aGMZzk8X"
   },
   "outputs": [],
   "source": [
    "def importance(original_image,x):\n",
    "  box_width, box_height, channels = x.shape\n",
    "  width, height = Image.open(original_image).size\n",
    "  return((box_height* box_width)/(height*width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yt6G6el09MW"
   },
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftIsKwBF0w3b"
   },
   "source": [
    "In conclusion, we have this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7Y_4Xq600G6"
   },
   "outputs": [],
   "source": [
    "def CropProcessing(image_path=None,mode=0):\n",
    "  img = Image.open(image_path)\n",
    "  ec = extract_coord(image_path)\n",
    "  plots = []\n",
    "  labels = []\n",
    "  coord = []\n",
    "  for i in range(len(ec)):\n",
    "    img2 = img.crop(ec.loc[i]['Coordinates'])\n",
    "    #Reshape image\n",
    "    #img2.thumbnail(size, Image.ANTIALIAS)\n",
    "    label = ec.loc[i]['Label']\n",
    "    labels.append(label)\n",
    "    plots.append(np.array(img2.convert('RGB')))\n",
    "    coord.append(ec.loc[i]['Coordinates'])\n",
    "    snd = pd.concat([pd.Series(labels),pd.Series(plots)],axis=1)\n",
    "    \n",
    "  for label in range(len(labels)):\n",
    "    if mode == \"SIFT\":\n",
    "      yield [image_path,snd.loc[label,0],importance(image_path,snd.loc[label,1]),SIFT_extract(snd.loc[label,1])]\n",
    "    elif mode == \"YOLO\":\n",
    "      yield [image_path,snd.loc[label,0],importance(image_path,snd.loc[label,1]),feature_extract_YOLO(snd.loc[label,1])]\n",
    "    elif mode == \"VGG\":\n",
    "      yield [image_path,snd.loc[label,0],importance(image_path,snd.loc[label,1]),feature_extract_VGG16(snd.loc[label,1])]\n",
    "    elif mode == 0: #Do not use any feature extraction method but return the crop\n",
    "      yield [image_path,snd.loc[label,0],snd.loc[label,1],importance(image_path,snd.loc[label,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VFEwIrP4Nkx"
   },
   "outputs": [],
   "source": [
    "#Example with VGG16\n",
    "\n",
    "# example1 = list(CropProcessing(\"/content/A/bald-eagle.jpg\",\n",
    "#                                mode = \"VGG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "xqYFo94i9Cj-",
    "outputId": "4ec11f26-f771-4d87-bbe7-3b47babbd065"
   },
   "outputs": [],
   "source": [
    "# #Showing the four separate descriptors for the crop\n",
    "# print('DESCRIPTORS:','\\n')\n",
    "# print(\"ID ORIGINAL IMAGE:\",example1[0][0],\"\\n\") #Show the original image identificator\n",
    "# print(\"CROP LABEL:\",example1[0][1],\"\\n\") #Show the label\n",
    "# #print(\"FEATURE:\",example1[0][2],\"\\n\") #Show the array of feature from MobileNetV2\n",
    "# print(\"FEATURE SHAPE:\",example1[0][2].shape,\"\\n\")\n",
    "# print(\"IMPORTANCE:\",example1[0][3]) #Show the importance score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcwXARNfA-5G"
   },
   "source": [
    "Thus, in output will be return the list of these descriptors for each crop extracted from the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qHiPlvg1EAa"
   },
   "source": [
    "###**4. Custom Data Loader**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLpm19zAOCyr"
   },
   "source": [
    "Per ogni immagine:\n",
    "  - Individua gli oggetti\n",
    "  - Estrae ogni crop\n",
    "    - Per ogni croop\n",
    "      - Genera il vettore di descrittori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIAt0BX-V0eQ"
   },
   "outputs": [],
   "source": [
    "def image_generator(directory, mode):\n",
    "  files = os.listdir(directory)\n",
    "  namefile = [os.path.join(directory, file) for file in files]\n",
    "  res = []\n",
    "  for i in range(len(namefile)-1):\n",
    "    res.append(list(CropProcessing(namefile[i],mode)))\n",
    "    break\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def my_image_generator(directory, mode):\n",
    "    print(mode)\n",
    "    files = os.listdir(directory)\n",
    "    namefile = [os.path.join(directory, file) for file in files]\n",
    "    N = len(namefile)\n",
    "    for i, file in enumerate(namefile):\n",
    "        print(\"%2.3f\" % (100 * i / N), end=\"\\r\")\n",
    "        crops = list(CropProcessing(file, mode))\n",
    "        if crops:\n",
    "            crops_df = pd.DataFrame(crops).dropna(axis=0)\n",
    "            if not len(crops_df.index):\n",
    "                continue\n",
    "            crops_df[3] = crops_df[3].map(lambda m: str(list([list(x) for x in m])) \\\n",
    "                                          .replace(\".0,\", \",\")\n",
    "                                          .replace(\" \", \"\"))\n",
    "            for i, row in crops_df.iterrows():\n",
    "                directory = \"./data/\" + mode + \"/\"\n",
    "                create_directory_if_not_exists(directory)\n",
    "                filename = row[1] + \".csv\"\n",
    "                with open(directory + filename, \"a\") as out_file_stream:\n",
    "                    out_file_stream.write(pd.DataFrame([row]).to_csv(header=False,\n",
    "                                                                     index=False,\n",
    "                                                                     columns=[0, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT\n",
      "99.980\r"
     ]
    }
   ],
   "source": [
    "for method in [\"SIFT\"]:\n",
    "    my_image_generator(\"./val2017\", method)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:dsim]",
   "language": "python",
   "name": "conda-env-dsim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
